{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive word2vec visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all steps needed to make a word2vec model from file with text sentences and then visualize that model in interactive Galaxy-like style using https://github.com/anvaka/pm library\n",
    "\n",
    "Assumptions:\n",
    "1. Lets assume you have the \"input.txt\" text file where sentences are divided by new lines. \n",
    "2. The distance between two words in word2vec model depends on frequency of these two words occuring in one sentences\n",
    "\n",
    "Contents of sample input.txt file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Friends, you are awesome\n",
    "I cant express how much I appreciate all your kind words and warm feedback\n",
    "It really means a world for me \n",
    "Thank you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my real world example (which unfortunately I cannot disclose) there are 131101 lines of phrases in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input file\n",
    "input_file = \"input.txt\"\n",
    "\n",
    "# this files are going to be generated during the script run\n",
    "phrases_file = \"phrases.file\"\n",
    "bin_file = 'bin.bin'\n",
    "clusters_file = \"clusters.file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from smart_open import smart_open\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in smart_open(self.filename, 'r'):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(input_file)\n",
    "model = gensim.models.Word2Vec(sentences)\n",
    "model.save(bin_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-09 22:03:18,067 : INFO : loading Word2Vec object from bin.bin\n",
      "2018-09-09 22:03:18,629 : INFO : loading wv recursively from bin.bin.wv.* with mmap=None\n",
      "2018-09-09 22:03:18,630 : INFO : loading vectors from bin.bin.wv.vectors.npy with mmap=None\n",
      "2018-09-09 22:03:18,726 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-09 22:03:18,737 : INFO : loading vocabulary recursively from bin.bin.vocabulary.* with mmap=None\n",
      "2018-09-09 22:03:18,746 : INFO : loading trainables recursively from bin.bin.trainables.* with mmap=None\n",
      "2018-09-09 22:03:18,752 : INFO : loading syn1neg from bin.bin.trainables.syn1neg.npy with mmap=None\n",
      "2018-09-09 22:03:18,868 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-09 22:03:18,869 : INFO : loaded bin.bin\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(bin_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready. You can play with it, looking at probabilites of co-occurence of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_output_word(['you']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131101"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/9e/dbfa8bad4015b23b1c2e29f1dc6178c3eec4e004a654ea7b38ba618998ec/annoy-1.13.0.tar.gz (634kB)\n",
      "\u001b[K    100% |████████████████████████████████| 634kB 85kB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Running setup.py bdist_wheel for annoy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/timming/Library/Caches/pip/wheels/96/af/26/f26df0a684b1e41ad8c56a13fc13e7a0a15a8a1a8b1cb0111a\n",
      "Successfully built annoy\n",
      "\u001b[31msmart-open 1.6.0 requires bz2file, which is not installed.\u001b[0m\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.13.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.31395504e-02,  1.32039428e-01,  1.98137254e-01, -6.41543150e-01,\n",
       "       -3.02360356e-01,  5.15950955e-02, -2.51930326e-01, -7.05299899e-03,\n",
       "        2.32723579e-01, -6.66235341e-03, -2.14652702e-01, -2.47196555e-01,\n",
       "        1.45640746e-01,  3.48122656e-01,  6.06271505e-01,  8.97369981e-02,\n",
       "        7.57813603e-02, -8.50206241e-02, -1.97893649e-01, -3.94424856e-01,\n",
       "        3.59753698e-01, -6.98147357e-01,  1.80305824e-01, -5.21175027e-01,\n",
       "       -6.20052926e-02, -4.78175551e-01, -4.65428419e-02, -1.77680984e-01,\n",
       "        3.55405957e-02, -2.69618422e-01, -3.99093367e-02, -7.88689435e-01,\n",
       "       -2.81666100e-01,  2.89484203e-01,  2.48964489e-01,  1.79778904e-01,\n",
       "       -1.17163353e-01,  5.55652194e-02, -4.98614879e-03,  1.64480507e-01,\n",
       "       -8.75537470e-03,  3.27218063e-02, -2.16997966e-01, -2.19280675e-01,\n",
       "        3.19542527e-01,  6.28549635e-01, -2.93996036e-01,  1.13070235e-01,\n",
       "       -2.29681745e-01,  1.86282262e-01, -5.71526885e-01,  9.04795766e-01,\n",
       "       -7.44460523e-02,  4.34337221e-02,  1.25443384e-01,  7.64962256e-01,\n",
       "        3.33262265e-01, -8.42389315e-02, -3.23805399e-02,  1.31305292e-01,\n",
       "        1.82901800e-01,  1.56667411e-01, -4.58106190e-01,  4.38938558e-01,\n",
       "        4.35334235e-01, -2.54311144e-01, -9.61083826e-03, -3.91256501e-04,\n",
       "        6.40498281e-01, -5.58744550e-01, -7.55105391e-02,  7.50568807e-02,\n",
       "        1.55764595e-01,  2.65959296e-02, -9.40529723e-03, -2.40905762e-01,\n",
       "       -2.85713792e-01,  3.50980401e-01,  1.37582451e-01, -1.27530977e-01,\n",
       "        1.74033850e-01, -5.26087999e-01, -2.99961660e-02,  2.55315244e-01,\n",
       "       -3.32466453e-01, -1.81982830e-01,  2.95719057e-01, -2.33691022e-01,\n",
       "       -2.51997143e-01,  4.45596814e-01,  5.02362549e-01,  4.21845838e-02,\n",
       "        1.67119697e-01,  1.09456964e-01,  1.43487349e-01,  2.94569433e-01,\n",
       "        4.97743368e-01, -5.32946587e-01,  4.63685125e-01, -1.63507402e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['someword']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build an index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "# Install from https://github.com/spotify/annoy\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Ignore all vectors with distance larger than this:\n",
    "threshold = 0.9\n",
    "\n",
    "# This file will contain nearest neighbors, one per line:\n",
    "# node [tab char] neighbor_1 neighbor_2 ...\n",
    "\n",
    "out_file = \"edges.txt\"\n",
    "\n",
    "# How many dimension in the vector space\n",
    "dimensions = 100\n",
    "\n",
    "# How many trees do want to use for `AnnoyIndex`\n",
    "max_trees = 50\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "\n",
    "word_id = 0\n",
    "word_index = AnnoyIndex(dimensions)\n",
    "words = []\n",
    "\n",
    "for word in vocab:\n",
    "    # There are a lot of words with numbers (dates, years) - and they are not very intersting to me.\n",
    "    # There are also ~140K instances of words with non-word characters, so we are going to ignore them\n",
    "    # as well\n",
    "    #if re.search('[0-9\\W]', word):\n",
    "    #    continue\n",
    "\n",
    "    words.append(word)\n",
    "    vectors = model[word]\n",
    "    word_index.add_item(word_id, vectors)\n",
    "    word_id += 1\n",
    "    \n",
    "word_index.build(max_trees)\n",
    "\n",
    "# If you want to save index:\n",
    "word_index.save('crawl_50_clean.ann')\n",
    "\n",
    "# If you want to load index:\n",
    "# u1 = AnnoyIndex(dimensions)\n",
    "# u1.load('crawl_50_clean.ann')\n",
    "\n",
    "# naive test:\n",
    "# result = word_index.get_nns_by_item(words.index('dog'), 42, include_distances=True)\n",
    "# result = zip(result[0], result[1])\n",
    "# print([(words[x], dist) for x, dist in result]) # will find the 1000 nearest neighbors\n",
    "\n",
    "out = open(out_file,'w')\n",
    "\n",
    "for idx in range(word_id):\n",
    "    try:\n",
    "        word = words[idx]\n",
    "        result = word_index.get_nns_by_item(idx, 42, include_distances=True)\n",
    "        pairs = zip(result[0], result[1])\n",
    "        edges = [words[pair[0]] for pair in pairs if (words[pair[0]] != word) and (pair[1] < threshold)]\n",
    "        if len(edges) > 0:\n",
    "            out.write(word + '\\t' + \" \".join(edges) + '\\n')\n",
    "\n",
    "        if idx % 10000 == 0:\n",
    "            print(idx)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(idx)\n",
    "\n",
    "out.close()\n",
    "\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready. Now we go to visualization part. All kudos go to anvaka and his https://github.com/anvaka/pm library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run `node edges2graph.js graph-data/edges.txt` - this will save graph in binary format into graph-data folder (graph-data/labels.json, graph-data/links.bin)\n",
    "\n",
    "Download and compile layout from https://github.com/anvaka/ngraph.native (valid for linux and macos)\n",
    "Then copy the binary to this project and run\n",
    "\n",
    "`./layout++ ./graph-data/links.bin`\n",
    "\n",
    "You will need to manually kill it (Ctrl + C) after 500-700 iterations.\n",
    "\n",
    "`\n",
    "mv 500.bin ./graph_data/positions.bin\n",
    "mv graph-data/* graph_data/`\n",
    "\n",
    "`http-server --cors`, before that do `npm install -g http-server`\n",
    "\n",
    "Clone pm from https://github.com/anvaka/pm\n",
    "\n",
    "in `src/config.js` write IP like this: \n",
    "\n",
    "```\n",
    "export default {\n",
    "  dataUrl: '//127.0.0.1:8080/'\n",
    "};\n",
    "```\n",
    "\n",
    "Run `npm install`\n",
    "Then `npm start`\n",
    "\n",
    "Open link http://127.0.0.1:8081/#/galaxy/graph_data?ml=150&s=1.75&l=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is going to be something like this:\n",
    "\n",
    "![Alt Text](https://elmiles.com/download/2019-06-06%2015-31-43.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
